{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy import stats \n",
    "from scipy.io.arff import loadarff\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#EEG (Preprocessing explained in another file)\n",
    "eeg_raw_data, meta = loadarff('EEG Eye State.arff')\n",
    "eeg_data = np.array(eeg_raw_data[meta.names()[0]].astype(float, copy = True)).reshape(14980,1)\n",
    "for i in range(1,len(meta.names())-1):\n",
    "    eeg_data = np.c_[eeg_data, np.array(eeg_raw_data[meta.names()[i]]).astype(float, copy = True)]\n",
    "eye_state = np.array(eeg_raw_data[meta.names()[len(meta.names())-1]].astype(int, copy = True)).reshape(14980,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define PCA Function\n",
    "def pca(data, num_of_prin_comp, data_orientation = \"row\"):\n",
    "    num_of_data = len(data)\n",
    "    dim_of_data = len(data[0])\n",
    "    if data_orientation == \"row\":\n",
    "        transposed_data = np.transpose(data) #Changes dataset so that data samples are column vectors\n",
    "    mean = transposed_data.mean(1)  #Mean Vector\n",
    "    centered_data = np.zeros((dim_of_data,num_of_data))\n",
    "\n",
    "    for i in range(num_of_data):\n",
    "        centered_data[:,i] = transposed_data[:,i] - mean  #Centering Data\n",
    "\n",
    "    svd_u, svd_sigma, svd_v = np.linalg.svd(centered_data, full_matrices = True)  # Singular Value Decompostion\n",
    "\n",
    "    u = np.zeros((dim_of_data,num_of_prin_comp))\n",
    "    s = np.zeros((num_of_prin_comp,num_of_prin_comp))\n",
    "\n",
    "    for i in range(dim_of_data):\n",
    "        for j in range(num_of_prin_comp):\n",
    "            u[i,j] = svd_u[i,j] #First r singular vectors of U\n",
    "    for i in range(num_of_prin_comp):\n",
    "        s[i,i] = svd_sigma[i] #Largest r singular values\n",
    "    \n",
    "    w = np.matrix(u)*np.matrix(s) #Principal Component Matrix with Principal Axes as Columns\n",
    "    for i in range(num_of_prin_comp):\n",
    "        w[:,i] = w[:,i]/np.linalg.norm(w[:,i]) #Normalizing Each Principal Component\n",
    "\n",
    "\n",
    "    transformed_data = np.transpose(np.transpose(w)*centered_data) #Feature Vectors\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is multidimensional, it could take much computational power to run the algorithm multiple times. That is why dimensionality reduction is used to simplify the data so that it represents 99% of the original data. This is done by the following equation:\n",
    "<br>\n",
    "$\\frac{\\sum_{i=1}^{k} \\sigma^2_i}{\\left \\| \\overline{X} \\right \\|^2_F}\\geq .95$\n",
    "<br>\n",
    "This uses the singular value decomposition and the correlation of representation to the singular vlaues to find the top r principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99% of the Original Data is represented by the top 3 principal components\n"
     ]
    }
   ],
   "source": [
    "#Finding the smallest number of principal components for .99 Representation of Original Data:\n",
    "m = len(eeg_data[0])\n",
    "n = len(eeg_data)\n",
    "centered = np.zeros((m,n))\n",
    "for i in range(n):\n",
    "    #Centering Training Data\n",
    "    centered[:,i] = np.transpose(eeg_data)[:,i] - np.transpose(eeg_data).mean(1)\n",
    "training_data_norm_squared = np.square(np.linalg.norm(centered))\n",
    "svd_u, svd_sigma, svd_v = np.linalg.svd(centered, full_matrices = True)  # SVD\n",
    "\n",
    "r = 0 #Top r principal components\n",
    "\n",
    "for i in range(len(svd_sigma)):\n",
    "    sum = 0\n",
    "    representation = 0;\n",
    "    for j in range(i+1):\n",
    "        sum += np.square(svd_sigma[j])\n",
    "    representation = sum/training_data_norm_squared\n",
    "    if representation >= .99:\n",
    "        r = i+1\n",
    "        print(\"99% of the Original Data is represented by the top\", r, \"principal components\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the top r principal components, use Principal Component Analysis to reduce the dimensions of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_eeg = pca(eeg_data, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Gaussian Naive Bayes Classifier Function assumming Uniform Prior Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian Naive Bayes Classifier with Uniform Prior Distribution\n",
    "import scipy.stats\n",
    "def gaussian_naive_bayes(num_of_classes, training_data, training_labels, test_data):\n",
    "    (n,m) = training_data.shape #n = number of training samples, m = dimensionality of data\n",
    "    \n",
    "    #Counting Training Data in Each Class\n",
    "    class_size = np.zeros((num_of_classes)) #Defining number of training data in each class\n",
    "    for i in range(n):\n",
    "        class_size[int(training_labels[i])] += 1\n",
    "    \n",
    "    #Seperating Training Data\n",
    "    seperated_training_data = np.zeros((num_of_classes), dtype = object)\n",
    "    for i in range(num_of_classes):\n",
    "        seperated_training_data[i] = np.zeros((int(class_size[i]), m))\n",
    "    count = np.zeros((num_of_classes))\n",
    "    for i in range(n):\n",
    "        #Seperating the training data into arrays containing data for each class for easier calculation of class mean and variance\n",
    "        seperated_training_data[int(training_labels[i])][int(count[int(training_labels[i])])] = training_data[i] \n",
    "        count[int(training_labels[i])] += 1\n",
    "    \n",
    "    #Mean and Variance Vectors\n",
    "    mean = np.zeros((num_of_classes, m))\n",
    "    variance = np.zeros((num_of_classes, m))\n",
    "    for i in range(num_of_classes):\n",
    "        mean[i] = seperated_training_data[i].mean(0) #Mean Vector for each class containing the mean of each feature in that class\n",
    "        variance[i] = np.var(seperated_training_data[i], axis=0) #Variance Vector for each class\n",
    "        \n",
    "    #Defining Gaussian Distribution PDF (1-Dimensional)\n",
    "    def pdf(x, me, var):\n",
    "        return (1/np.sqrt(2*math.pi*var))*np.exp(-np.square(x-me)/(2*var))\n",
    "        \n",
    "    #Classifying Test Data\n",
    "    classified_labels = np.ones((len(test_data)))\n",
    "    for i in range(len(test_data)):\n",
    "        probabilities = np.ones((num_of_classes)) #Contains the probability that test data belongs to each class\n",
    "        for j in range(num_of_classes):\n",
    "            for k in range(m):\n",
    "                #Muptiplies all the probabilities of features conditioned on class\n",
    "                #probabilities[j] *= pdf(test_data[i,k], mean[j][k], variance[j][k]) \n",
    "                probabilities[j] *= scipy.stats.norm(mean[j][k], variance[j][k]).pdf(test_data[i,k])\n",
    "        classified_labels[i] = int(np.argmax(probabilities)) # Classifies the test data on the class with highest probability\n",
    "        #print(probabilities, classified_labels[i], training_labels[i], classified_labels[i]==training_labels[i])\n",
    "    \n",
    "    return classified_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function that takes split data and labels to perform k-fold cross validation and return the mean and variance of the error on all the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Vaidation Function\n",
    "def cross_validation(k, split_data, split_labels, classifier_func = gaussian_naive_bayes):\n",
    "    classification_rate = np.zeros((k)) #Array to hold classification rate of each fold\n",
    "    \n",
    "    for i in range(k):\n",
    "        #Seperating split data into training and test sets (4 splits for training, 1 for testing)\n",
    "        training = np.concatenate(np.delete(split_data, i))\n",
    "        train_labels = np.concatenate(np.delete(split_labels, i))\n",
    "        \n",
    "        test = split_data[i]\n",
    "        test_labels = split_labels[i]\n",
    "        \n",
    "        #Obtaining classified test labels\n",
    "        if (classifier_func == \"sklearn\"):\n",
    "            gnb = GaussianNB()\n",
    "            gnb.fit(training, train_labels)\n",
    "            classifier_labels = gnb.predict(test)\n",
    "        else:\n",
    "            classifier_labels = classifier_func(2, training, train_labels, test)\n",
    "        \n",
    "        #Calculating classification rate: (# of correctly classified test samples)/(total number of test samples)\n",
    "        for j in range(len(classifier_labels)):\n",
    "            if test_labels[j] == classifier_labels[j]:\n",
    "                classification_rate[i] += 1\n",
    "        classification_rate[i] /= len(classifier_labels)\n",
    "        \n",
    "    #Returning the mean and variance of the classification rates\n",
    "    return np.mean(classification_rate), np.var(classification_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into 10 folds for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "n=len(eeg_data)\n",
    "\n",
    "label_splits = np.zeros((k), dtype=object)\n",
    "transformed_data_splits = np.zeros((k), dtype=object)\n",
    "\n",
    "#Defining the splits\n",
    "for i in range(0,k):\n",
    "    label_splits[i] = np.array(eye_state[int((i*n)/k): int((i+1)*n/k)])\n",
    "    transformed_data_splits[i] = np.array(transformed_eeg[int((i*n)/k): int((i+1)*n/k)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of classification rate is:  0.4488651535380508\n",
      "Variance of classification rate is:  0.0817910485008048\n"
     ]
    }
   ],
   "source": [
    "mean, var = cross_validation(k, transformed_data_splits, label_splits)\n",
    "print(\"Mean of classification rate is: \", mean)\n",
    "print(\"Variance of classification rate is: \", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
