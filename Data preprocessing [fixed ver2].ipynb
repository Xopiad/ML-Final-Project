{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io.arff import loadarff\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data - EEG Eye State data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the EEG Eye State data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data. The function loadarff read most arff files and it can also read\n",
    "# files with missing data, representing the data points as NaNs. This \n",
    "# information is important for data preprocessing. The data used here \n",
    "# has no missing values\n",
    "EEG_Eye_State, meta = loadarff('EEGEyeState.arff')\n",
    "\n",
    "# number of attributes are 14\n",
    "# number of samples are 14980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset: EEG_DATA\n",
       "\tAF3's type is numeric\n",
       "\tF7's type is numeric\n",
       "\tF3's type is numeric\n",
       "\tFC5's type is numeric\n",
       "\tT7's type is numeric\n",
       "\tP7's type is numeric\n",
       "\tO1's type is numeric\n",
       "\tO2's type is numeric\n",
       "\tP8's type is numeric\n",
       "\tT8's type is numeric\n",
       "\tFC6's type is numeric\n",
       "\tF4's type is numeric\n",
       "\tF8's type is numeric\n",
       "\tAF4's type is numeric\n",
       "\teyeDetection's type is nominal, range is ('0', '1')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# meta contains information about the arff file, as shown below is the attributes\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EEG_Eye_State records the data of the arff file, accessible by attribute names\n",
    "# When add the EEG_Eye_State data to matrix, each element in the matrix has the type numpy.bytes_, therefore need to convert to\n",
    "# float or int type so data matrix could be manipulated without errors\n",
    "# Turn EEG_Eye_State into matrix of data\n",
    "Eye_State_data = np.array(EEG_Eye_State[meta.names()[0]].astype(float, copy = True)).reshape(14980,1)\n",
    "\n",
    "# Add every input vector to Eye_State_data\n",
    "for i in range(1,len(meta.names())-1):\n",
    "    Eye_State_data = np.c_[Eye_State_data, np.array(EEG_Eye_State[meta.names()[i]]).astype(float, copy = True)]\n",
    "\n",
    "# Label vector, including 0 or 1\n",
    "Eye_State_label = np.array(EEG_Eye_State[meta.names()[len(meta.names())-1]].astype(int, copy = True)).reshape(14980,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4329.23 4009.23 4289.23 4148.21 4350.26 4586.15 4096.92 4641.03 4222.05\n",
      "  4238.46 4211.28 4280.51 4635.9  4393.85]\n",
      " [4324.62 4004.62 4293.85 4148.72 4342.05 4586.67 4097.44 4638.97 4210.77\n",
      "  4226.67 4207.69 4279.49 4632.82 4384.1 ]\n",
      " [4327.69 4006.67 4295.38 4156.41 4336.92 4583.59 4096.92 4630.26 4207.69\n",
      "  4222.05 4206.67 4282.05 4628.72 4389.23]]\n"
     ]
    }
   ],
   "source": [
    "print(Eye_State_data[0:3,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data - Autism Adult data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Autism Adult data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data. The function loadarff read most arff files and it can also read\n",
    "# files with missing data, representing the data points as NaNs. This \n",
    "# information is important for data preprocessing. The data used here \n",
    "# has some missing values. Need to find those 'missing data samples'.\n",
    "Autism_Adult, meta = loadarff('AutismAdultData.arff')\n",
    "\n",
    "# number of attributes are 21\n",
    "# number of samples are 704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset: adult-weka.filters.unsupervised.attribute.NumericToNominal-Rfirst-10\n",
       "\tA1_Score's type is nominal, range is ('0', '1')\n",
       "\tA2_Score's type is nominal, range is ('0', '1')\n",
       "\tA3_Score's type is nominal, range is ('0', '1')\n",
       "\tA4_Score's type is nominal, range is ('0', '1')\n",
       "\tA5_Score's type is nominal, range is ('0', '1')\n",
       "\tA6_Score's type is nominal, range is ('0', '1')\n",
       "\tA7_Score's type is nominal, range is ('0', '1')\n",
       "\tA8_Score's type is nominal, range is ('0', '1')\n",
       "\tA9_Score's type is nominal, range is ('0', '1')\n",
       "\tA10_Score's type is nominal, range is ('0', '1')\n",
       "\tage's type is numeric\n",
       "\tgender's type is nominal, range is ('f', 'm')\n",
       "\tethnicity's type is nominal, range is ('White-European', 'Latino', 'Others', 'Black', 'Asian', \"'Middle Eastern '\", 'Pasifika', \"'South Asian'\", 'Hispanic', 'Turkish', 'others')\n",
       "\tjundice's type is nominal, range is ('no', 'yes')\n",
       "\taustim's type is nominal, range is ('no', 'yes')\n",
       "\tcontry_of_res's type is nominal, range is (\"'United States'\", 'Brazil', 'Spain', 'Egypt', \"'New Zealand'\", 'Bahamas', 'Burundi', 'Austria', 'Argentina', 'Jordan', 'Ireland', \"'United Arab Emirates'\", 'Afghanistan', 'Lebanon', \"'United Kingdom'\", \"'South Africa'\", 'Italy', 'Pakistan', 'Bangladesh', 'Chile', 'France', 'China', 'Australia', 'Canada', \"'Saudi Arabia'\", 'Netherlands', 'Romania', 'Sweden', 'Tonga', 'Oman', 'India', 'Philippines', \"'Sri Lanka'\", \"'Sierra Leone'\", 'Ethiopia', \"'Viet Nam'\", 'Iran', \"'Costa Rica'\", 'Germany', 'Mexico', 'Russia', 'Armenia', 'Iceland', 'Nicaragua', \"'Hong Kong'\", 'Japan', 'Ukraine', 'Kazakhstan', 'AmericanSamoa', 'Uruguay', 'Serbia', 'Portugal', 'Malaysia', 'Ecuador', 'Niger', 'Belgium', 'Bolivia', 'Aruba', 'Finland', 'Turkey', 'Nepal', 'Indonesia', 'Angola', 'Azerbaijan', 'Iraq', \"'Czech Republic'\", 'Cyprus')\n",
       "\tused_app_before's type is nominal, range is ('no', 'yes')\n",
       "\tresult's type is numeric\n",
       "\tage_desc's type is nominal, range is (\"'18 and more'\",)\n",
       "\trelation's type is nominal, range is ('Self', 'Parent', \"'Health care professional'\", 'Relative', 'Others')\n",
       "\tClass/ASD's type is nominal, range is ('NO', 'YES')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# meta contains information about the arff file, as shown below is the attributes\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1_Score</th>\n",
       "      <th>A2_Score</th>\n",
       "      <th>A3_Score</th>\n",
       "      <th>A4_Score</th>\n",
       "      <th>A5_Score</th>\n",
       "      <th>A6_Score</th>\n",
       "      <th>A7_Score</th>\n",
       "      <th>A8_Score</th>\n",
       "      <th>A9_Score</th>\n",
       "      <th>A10_Score</th>\n",
       "      <th>...</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>jundice</th>\n",
       "      <th>austim</th>\n",
       "      <th>contry_of_res</th>\n",
       "      <th>used_app_before</th>\n",
       "      <th>result</th>\n",
       "      <th>age_desc</th>\n",
       "      <th>relation</th>\n",
       "      <th>Class/ASD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>White-European</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>'United States'</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>m</td>\n",
       "      <td>Latino</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>m</td>\n",
       "      <td>Latino</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>Spain</td>\n",
       "      <td>no</td>\n",
       "      <td>8</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Parent</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>White-European</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>'United States'</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>m</td>\n",
       "      <td>Others</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>'United States'</td>\n",
       "      <td>no</td>\n",
       "      <td>9</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>Black</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>'United States'</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>m</td>\n",
       "      <td>White-European</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>'New Zealand'</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Parent</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>m</td>\n",
       "      <td>White-European</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>'United States'</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>m</td>\n",
       "      <td>Asian</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>Bahamas</td>\n",
       "      <td>no</td>\n",
       "      <td>8</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>'Health care professional'</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  A1_Score A2_Score A3_Score A4_Score A5_Score A6_Score A7_Score A8_Score  \\\n",
       "0        1        1        1        1        0        0        1        1   \n",
       "1        1        1        0        1        0        0        0        1   \n",
       "2        1        1        0        1        1        0        1        1   \n",
       "3        1        1        0        1        0        0        1        1   \n",
       "4        1        0        0        0        0        0        0        1   \n",
       "5        1        1        1        1        1        0        1        1   \n",
       "6        0        1        0        0        0        0        0        1   \n",
       "7        1        1        1        1        0        0        0        0   \n",
       "8        1        1        0        0        1        0        0        1   \n",
       "9        1        1        1        1        0        1        1        1   \n",
       "\n",
       "  A9_Score A10_Score    ...    gender       ethnicity jundice austim  \\\n",
       "0        0         0    ...         f  White-European      no     no   \n",
       "1        0         1    ...         m          Latino      no    yes   \n",
       "2        1         1    ...         m          Latino     yes    yes   \n",
       "3        0         1    ...         f  White-European      no    yes   \n",
       "4        0         0    ...         f             NaN      no     no   \n",
       "5        1         1    ...         m          Others     yes     no   \n",
       "6        0         0    ...         f           Black      no     no   \n",
       "7        1         0    ...         m  White-European      no     no   \n",
       "8        1         1    ...         m  White-European      no     no   \n",
       "9        1         0    ...         m           Asian     yes    yes   \n",
       "\n",
       "     contry_of_res used_app_before result       age_desc  \\\n",
       "0  'United States'              no      6  '18 and more'   \n",
       "1           Brazil              no      5  '18 and more'   \n",
       "2            Spain              no      8  '18 and more'   \n",
       "3  'United States'              no      6  '18 and more'   \n",
       "4            Egypt              no      2  '18 and more'   \n",
       "5  'United States'              no      9  '18 and more'   \n",
       "6  'United States'              no      2  '18 and more'   \n",
       "7    'New Zealand'              no      5  '18 and more'   \n",
       "8  'United States'              no      6  '18 and more'   \n",
       "9          Bahamas              no      8  '18 and more'   \n",
       "\n",
       "                     relation Class/ASD  \n",
       "0                        Self        NO  \n",
       "1                        Self        NO  \n",
       "2                      Parent       YES  \n",
       "3                        Self        NO  \n",
       "4                         NaN        NO  \n",
       "5                        Self       YES  \n",
       "6                        Self        NO  \n",
       "7                      Parent        NO  \n",
       "8                        Self        NO  \n",
       "9  'Health care professional'       YES  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Autism_Adult records the data of the arff file, accessible by attribute names\n",
    "# When add the Autism_Adult data to matrix, each element in the matrix has the type numpy.bytes_, therefore need to convert to\n",
    "# int or str type so data matrix could be manipulated without errors\n",
    "# Turn Autism_Adult into matrix of data\n",
    "Autism_Adult_data = np.array(Autism_Adult[meta.names()[0]].astype(int, copy = True)).reshape(704,1)\n",
    "\n",
    "# Add every integer input vector to Eye_State_data\n",
    "for i in range(1,11):\n",
    "    Autism_Adult_data = np.c_[Autism_Adult_data, np.array(Autism_Adult[meta.names()[i]]).astype(int, copy = True)]\n",
    "\n",
    "# Add every string input vector to Eye_State_data\n",
    "for i in range(11,17):\n",
    "    Autism_Adult_data = np.c_[Autism_Adult_data, np.array(Autism_Adult[meta.names()[i]]).astype(str, copy = True)]\n",
    "\n",
    "# Add integer input vector to Eye_State_data, 18th column\n",
    "Autism_Adult_data = np.c_[Autism_Adult_data, np.array(Autism_Adult[meta.names()[17]]).astype(int, copy = True)]\n",
    "\n",
    "for i in range(18,len(meta.names())):\n",
    "    Autism_Adult_data = np.c_[Autism_Adult_data, np.array(Autism_Adult[meta.names()[i]]).astype(str, copy = True)]\n",
    "\n",
    "# convert to DataFrame so manipulation be easily done \n",
    "Autism_frame = pd.DataFrame(data = Autism_Adult_data, columns = meta.names()[:])\n",
    "\n",
    "# replace '?' with NaN, help to find columns of missing values\n",
    "Autism_frame.replace('?',np.NaN, inplace = True)\n",
    "\n",
    "# show the first 10 rows of data frame\n",
    "Autism_frame.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Discussion__ about preprocessing this data. As can be seen from the table above, there are some missing data points in the table. There are a few approaches: first, the missing values could take the _mean_ along the axis, here the axis should be the column. Second, the missing values might take the _median_ along the column or third, the _most frequent_. Unfortunately, all the missing values are in the _categorical_ input, thus _mean_ and _median_ could not be used. Mean of 'Latino' and 'Black' would make no numerical nor categorical sense here. The final solution is to use the _most frequent_. In fact, the row of missing values could be eliminated but given that there are only 704 samples and some information of that row might be very important to our model, elimination seems not to be a good idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the columns that have the missing values. As shown in table above, missing values are marked \"NaN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ethnicity', 'relation'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# print the columns that has the missing values\n",
    "print(Autism_frame.columns[Autism_frame.isnull().any()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_ethnicity_ is 13th columns and _relation_ is 20th column. Now find the most frequent value is each column and assign that value to the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the mode ( or most frequent value ) in each column\n",
    "autism_mode = Autism_frame.mode(axis=0)\n",
    "\n",
    "# assign most frequent value in column 'ethnicity' to the missing values\n",
    "Autism_frame[meta.names()[12]].replace(np.NaN, autism_mode[meta.names()[12]].values[0],inplace = True)\n",
    "\n",
    "# assign most frequent value in column 'relation' to the missing values\n",
    "Autism_frame[meta.names()[19]].replace(np.NaN, autism_mode[meta.names()[19]].values[0],inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are categorical data in our matrix, there is a need to encode them so ML algorithms could be implemented on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelEncoder_X = LabelEncoder()\n",
    "# encode only categorical attributes\n",
    "for i in range(11,len(meta.names())):\n",
    "    # since column 'result' is numeric, no encoding is needed for column 18th\n",
    "    if i != 17: \n",
    "        Autism_frame[meta.names()[i]] = labelEncoder_X.fit_transform(Autism_frame[meta.names()[i]])\n",
    "\n",
    "# Create data matrix for Autism Adult data\n",
    "Autism_matrix = Autism_frame.iloc[:,:-1].values\n",
    "Autism_label = Autism_frame.iloc[:,20].values\n",
    "# Convert to integers\n",
    "Autism_matrix = Autism_matrix.astype(int, copy = True)\n",
    "Autism_label = Autism_label.astype(int, copy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data - Air Quality UCI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Air Quality UCI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>CO(GT)</th>\n",
       "      <th>PT08.S1(CO)</th>\n",
       "      <th>NMHC(GT)</th>\n",
       "      <th>C6H6(GT)</th>\n",
       "      <th>PT08.S2(NMHC)</th>\n",
       "      <th>NOx(GT)</th>\n",
       "      <th>PT08.S3(NOx)</th>\n",
       "      <th>NO2(GT)</th>\n",
       "      <th>PT08.S4(NO2)</th>\n",
       "      <th>PT08.S5(O3)</th>\n",
       "      <th>T</th>\n",
       "      <th>RH</th>\n",
       "      <th>AH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-03-10</td>\n",
       "      <td>18:00:00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1360.00</td>\n",
       "      <td>150</td>\n",
       "      <td>11.881723</td>\n",
       "      <td>1045.50</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1056.25</td>\n",
       "      <td>113.0</td>\n",
       "      <td>1692.00</td>\n",
       "      <td>1267.50</td>\n",
       "      <td>13.600</td>\n",
       "      <td>48.875001</td>\n",
       "      <td>0.757754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-03-10</td>\n",
       "      <td>19:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1292.25</td>\n",
       "      <td>112</td>\n",
       "      <td>9.397165</td>\n",
       "      <td>954.75</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1173.75</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1558.75</td>\n",
       "      <td>972.25</td>\n",
       "      <td>13.300</td>\n",
       "      <td>47.700000</td>\n",
       "      <td>0.725487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-03-10</td>\n",
       "      <td>20:00:00</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1402.00</td>\n",
       "      <td>88</td>\n",
       "      <td>8.997817</td>\n",
       "      <td>939.25</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1140.00</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1554.50</td>\n",
       "      <td>1074.00</td>\n",
       "      <td>11.900</td>\n",
       "      <td>53.975000</td>\n",
       "      <td>0.750239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-03-10</td>\n",
       "      <td>21:00:00</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1375.50</td>\n",
       "      <td>80</td>\n",
       "      <td>9.228796</td>\n",
       "      <td>948.25</td>\n",
       "      <td>172.0</td>\n",
       "      <td>1092.00</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1583.75</td>\n",
       "      <td>1203.25</td>\n",
       "      <td>11.000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.786713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-03-10</td>\n",
       "      <td>22:00:00</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1272.25</td>\n",
       "      <td>51</td>\n",
       "      <td>6.518224</td>\n",
       "      <td>835.50</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1205.00</td>\n",
       "      <td>116.0</td>\n",
       "      <td>1490.00</td>\n",
       "      <td>1110.00</td>\n",
       "      <td>11.150</td>\n",
       "      <td>59.575001</td>\n",
       "      <td>0.788794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2004-03-10</td>\n",
       "      <td>23:00:00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1197.00</td>\n",
       "      <td>38</td>\n",
       "      <td>4.741012</td>\n",
       "      <td>750.25</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1336.50</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1393.00</td>\n",
       "      <td>949.25</td>\n",
       "      <td>11.175</td>\n",
       "      <td>59.175000</td>\n",
       "      <td>0.784772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2004-03-11</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1185.00</td>\n",
       "      <td>31</td>\n",
       "      <td>3.624399</td>\n",
       "      <td>689.50</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1461.75</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1332.75</td>\n",
       "      <td>732.50</td>\n",
       "      <td>11.325</td>\n",
       "      <td>56.775000</td>\n",
       "      <td>0.760312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2004-03-11</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1136.25</td>\n",
       "      <td>31</td>\n",
       "      <td>3.326677</td>\n",
       "      <td>672.00</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1453.25</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1332.75</td>\n",
       "      <td>729.50</td>\n",
       "      <td>10.675</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.770238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2004-03-11</td>\n",
       "      <td>02:00:00</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1094.00</td>\n",
       "      <td>24</td>\n",
       "      <td>2.339416</td>\n",
       "      <td>608.50</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1579.00</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1276.00</td>\n",
       "      <td>619.50</td>\n",
       "      <td>10.650</td>\n",
       "      <td>59.674999</td>\n",
       "      <td>0.764819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2004-03-11</td>\n",
       "      <td>03:00:00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1009.75</td>\n",
       "      <td>19</td>\n",
       "      <td>1.696658</td>\n",
       "      <td>560.75</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>1705.00</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>1234.75</td>\n",
       "      <td>501.25</td>\n",
       "      <td>10.250</td>\n",
       "      <td>60.200001</td>\n",
       "      <td>0.751657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date      Time  CO(GT)  PT08.S1(CO)  NMHC(GT)   C6H6(GT)  \\\n",
       "0 2004-03-10  18:00:00     2.6      1360.00       150  11.881723   \n",
       "1 2004-03-10  19:00:00     2.0      1292.25       112   9.397165   \n",
       "2 2004-03-10  20:00:00     2.2      1402.00        88   8.997817   \n",
       "3 2004-03-10  21:00:00     2.2      1375.50        80   9.228796   \n",
       "4 2004-03-10  22:00:00     1.6      1272.25        51   6.518224   \n",
       "5 2004-03-10  23:00:00     1.2      1197.00        38   4.741012   \n",
       "6 2004-03-11  00:00:00     1.2      1185.00        31   3.624399   \n",
       "7 2004-03-11  01:00:00     1.0      1136.25        31   3.326677   \n",
       "8 2004-03-11  02:00:00     0.9      1094.00        24   2.339416   \n",
       "9 2004-03-11  03:00:00     0.6      1009.75        19   1.696658   \n",
       "\n",
       "   PT08.S2(NMHC)  NOx(GT)  PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)  \\\n",
       "0        1045.50    166.0       1056.25    113.0       1692.00      1267.50   \n",
       "1         954.75    103.0       1173.75     92.0       1558.75       972.25   \n",
       "2         939.25    131.0       1140.00    114.0       1554.50      1074.00   \n",
       "3         948.25    172.0       1092.00    122.0       1583.75      1203.25   \n",
       "4         835.50    131.0       1205.00    116.0       1490.00      1110.00   \n",
       "5         750.25     89.0       1336.50     96.0       1393.00       949.25   \n",
       "6         689.50     62.0       1461.75     77.0       1332.75       732.50   \n",
       "7         672.00     62.0       1453.25     76.0       1332.75       729.50   \n",
       "8         608.50     45.0       1579.00     60.0       1276.00       619.50   \n",
       "9         560.75   -200.0       1705.00   -200.0       1234.75       501.25   \n",
       "\n",
       "        T         RH        AH  \n",
       "0  13.600  48.875001  0.757754  \n",
       "1  13.300  47.700000  0.725487  \n",
       "2  11.900  53.975000  0.750239  \n",
       "3  11.000  60.000000  0.786713  \n",
       "4  11.150  59.575001  0.788794  \n",
       "5  11.175  59.175000  0.784772  \n",
       "6  11.325  56.775000  0.760312  \n",
       "7  10.675  60.000000  0.770238  \n",
       "8  10.650  59.674999  0.764819  \n",
       "9  10.250  60.200001  0.751657  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data. The function read_csv read excel files and it can also read\n",
    "# files with missing data, representing the data points as NaNs. This \n",
    "# information is important for data preprocessing. The data used here \n",
    "# has some missing values. Need to find those 'missing data samples'.\n",
    "Air_Quality = pd.read_excel('AirQualityUCI.xlsx')\n",
    "\n",
    "# show the first 10 rows of data frame\n",
    "Air_Quality.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Discussion__ about preprocessing this data. The columns 'Date' and 'Time' are not used, therefore discarded. The missing values in this data are marked '-200'. There are a few approaches: first, the missing values could take the _mean_ along the axis, here the axis should be the column. Second, the missing values might take the _median_ along the column or third, the _most frequent_. Since the data's type is numerical, the _mean_ and _median_ can be used. Here _mean_ approach is utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete columns 'Date' and 'Time'\n",
    "Air_Quality.drop(columns = ['Date', 'Time'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column    Number of Missing Values\n",
      "1         1683\n",
      "2         366\n",
      "3         8443\n",
      "4         366\n",
      "5         366\n",
      "6         1639\n",
      "7         366\n",
      "8         1642\n",
      "9         366\n",
      "10         366\n",
      "11         366\n",
      "12         366\n",
      "13         366\n"
     ]
    }
   ],
   "source": [
    "# Count how many missing values does this data have\n",
    "print(\"Column    Number of Missing Values\")\n",
    "      \n",
    "for i in range(len(Air_Quality.columns)):\n",
    "    count = (Air_Quality[Air_Quality.columns[i]]==-200).value_counts()[1]\n",
    "    print(\"{}         {}\".format(i+1,count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sensor responsible for 'NMHC (GT)' must be really bad, yielding too many missing or bad values. Therefore, need to discard from data. the 1st, 6th and 8th also yield many bad values, however, we would use the _mean_ approach dicussed above to replace the bad values. 11th column is the output temperature and it has 366 bad values. For the output, using the approaches above would not be a good idea. Also, 366 appears too often, therefore, the measurements must be wrong at those missing values. Eliminating all the rows that have missing values at output should be the optimal choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CO(GT)</th>\n",
       "      <th>PT08.S1(CO)</th>\n",
       "      <th>NMHC(GT)</th>\n",
       "      <th>C6H6(GT)</th>\n",
       "      <th>PT08.S2(NMHC)</th>\n",
       "      <th>NOx(GT)</th>\n",
       "      <th>PT08.S3(NOx)</th>\n",
       "      <th>NO2(GT)</th>\n",
       "      <th>PT08.S4(NO2)</th>\n",
       "      <th>PT08.S5(O3)</th>\n",
       "      <th>T</th>\n",
       "      <th>RH</th>\n",
       "      <th>AH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>1.7</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>222</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>1.9</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>197</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>2.3</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>319</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>137</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>2.4</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>189</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>1.8</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>159</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>80</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>66</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>87</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>0.9</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>79</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     CO(GT)  PT08.S1(CO)  NMHC(GT)  C6H6(GT)  PT08.S2(NMHC)  NOx(GT)  \\\n",
       "524     1.7       -200.0       222    -200.0         -200.0     99.0   \n",
       "525     1.9       -200.0       197    -200.0         -200.0    108.0   \n",
       "526     2.3       -200.0       319    -200.0         -200.0    131.0   \n",
       "701     2.0       -200.0       137    -200.0         -200.0    129.0   \n",
       "702     2.4       -200.0       189    -200.0         -200.0    154.0   \n",
       "703     1.8       -200.0       159    -200.0         -200.0    118.0   \n",
       "704     1.0       -200.0        80    -200.0         -200.0     69.0   \n",
       "705     1.0       -200.0        66    -200.0         -200.0   -200.0   \n",
       "706     1.0       -200.0        87    -200.0         -200.0     97.0   \n",
       "707     0.9       -200.0        79    -200.0         -200.0    145.0   \n",
       "\n",
       "     PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)      T     RH     AH  \n",
       "524        -200.0     72.0        -200.0       -200.0 -200.0 -200.0 -200.0  \n",
       "525        -200.0     81.0        -200.0       -200.0 -200.0 -200.0 -200.0  \n",
       "526        -200.0     93.0        -200.0       -200.0 -200.0 -200.0 -200.0  \n",
       "701        -200.0    106.0        -200.0       -200.0 -200.0 -200.0 -200.0  \n",
       "702        -200.0    109.0        -200.0       -200.0 -200.0 -200.0 -200.0  \n",
       "703        -200.0     97.0        -200.0       -200.0 -200.0 -200.0 -200.0  \n",
       "704        -200.0     83.0        -200.0       -200.0 -200.0 -200.0 -200.0  \n",
       "705        -200.0   -200.0        -200.0       -200.0 -200.0 -200.0 -200.0  \n",
       "706        -200.0     79.0        -200.0       -200.0 -200.0 -200.0 -200.0  \n",
       "707        -200.0     84.0        -200.0       -200.0 -200.0 -200.0 -200.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show some rows that have missing values marked by '-200'\n",
    "Air_Quality[Air_Quality['T'] == -200].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the rows that have missing values at the output column vector\n",
    "Air_Quality.drop(index = Air_Quality[Air_Quality['T'] == -200].index, inplace = True)\n",
    "\n",
    "# Delete columns 'NMHC(GT)''\n",
    "Air_Quality.drop(columns = ['NMHC(GT)'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dropping unnecessary column and row, count the number of missing values again. It can be easily\n",
    "seen that our assumption was right, \"366 appears too often, therefore, the measurements must be wrong at those missing values. Eliminating all the rows that have missing values at output should be the optimal choice.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column    Number of Missing Values\n",
      "1         1647\n",
      "2         0\n",
      "3         0\n",
      "4         0\n",
      "5         1595\n",
      "6         0\n",
      "7         1598\n",
      "8         0\n",
      "9         0\n",
      "10         0\n",
      "11         0\n",
      "12         0\n"
     ]
    }
   ],
   "source": [
    "# Count how many missing values does this data have\n",
    "print(\"Column    Number of Missing Values\")     \n",
    "for i in range(len(Air_Quality.columns)):\n",
    "    count = Air_Quality.shape[0] - (Air_Quality[Air_Quality.columns[i]]==-200).value_counts().to_dict()[0]\n",
    "    print(\"{}         {}\".format(i+1,count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether to drop column 1, 5 and 7 or not would somehow affect out Machine Learning model. Dropping 1595 samples is dropping 2/9 our data. Dropping columns 1,5 and 7 leaves us with 8 attributes. Which drop gives us more total benefit is worth considering. Here, I choose to drop 1595 rows and first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the rows that have missing values at the output column vector \n",
    "Air_Quality.drop(index = Air_Quality[Air_Quality['NOx(GT)'] == -200].index, inplace = True)\n",
    "\n",
    "# Delete columns 'CO(GT)'\n",
    "Air_Quality.drop(columns = ['CO(GT)'], inplace = True)\n",
    "\n",
    "# number of attributes are 10\n",
    "# number of samples are 7396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column    Number of Missing Values\n",
      "1         0\n",
      "2         0\n",
      "3         0\n",
      "4         0\n",
      "5         0\n",
      "6         3\n",
      "7         0\n",
      "8         0\n",
      "9         0\n",
      "10         0\n",
      "11         0\n"
     ]
    }
   ],
   "source": [
    "# Count how many missing values does this data have\n",
    "print(\"Column    Number of Missing Values\")\n",
    "      \n",
    "for i in range(len(Air_Quality.columns)):\n",
    "    count = Air_Quality.shape[0] - (Air_Quality[Air_Quality.columns[i]]==-200).value_counts().to_dict()[0]\n",
    "    print(\"{}         {}\".format(i+1,count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For column 6, the _mean_ approach is utilized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data matrix for Air Qyality data\n",
    "Air_Quality_matrix = Air_Quality.iloc[:,Air_Quality.columns != 'T'].values\n",
    "Air_Quality_values = Air_Quality['T'].values\n",
    "\n",
    "# Taking care of missing values\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values = -200, strategy = 'mean', axis = 0)\n",
    "imputer = imputer.fit(Air_Quality_matrix[:,5:6])\n",
    "Air_Quality_matrix[:,5:6] = imputer.transform(Air_Quality_matrix[:,5:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
