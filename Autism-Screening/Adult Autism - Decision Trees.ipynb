{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and Preprocessing the Adult Autism Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy import stats \n",
    "from scipy.io.arff import loadarff\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#Autism\n",
    "Autism_Adult, meta = loadarff('Autism-Adult-Data.arff')\n",
    "\n",
    "Autism_Adult_data = np.array(Autism_Adult[meta.names()[0]].astype(int, copy = True)).reshape(704,1)\n",
    "\n",
    "# Add every integer input vector to Eye_State_data\n",
    "for i in range(1,11):\n",
    "    Autism_Adult_data = np.c_[Autism_Adult_data, np.array(Autism_Adult[meta.names()[i]]).astype(int, copy = True)]\n",
    "\n",
    "# Add every string input vector to Eye_State_data\n",
    "for i in range(11,17):\n",
    "    Autism_Adult_data = np.c_[Autism_Adult_data, np.array(Autism_Adult[meta.names()[i]]).astype(str, copy = True)]\n",
    "\n",
    "# Add integer input vector to Eye_State_data, 18th column\n",
    "Autism_Adult_data = np.c_[Autism_Adult_data, np.array(Autism_Adult[meta.names()[17]]).astype(int, copy = True)]\n",
    "\n",
    "for i in range(18,len(meta.names())):\n",
    "    Autism_Adult_data = np.c_[Autism_Adult_data, np.array(Autism_Adult[meta.names()[i]]).astype(str, copy = True)]\n",
    "\n",
    "Autism_frame = pd.DataFrame(data = Autism_Adult_data, columns = meta.names()[:])\n",
    "Autism_frame.replace('?',np.NaN, inplace = True)\n",
    "\n",
    "autism_mode = Autism_frame.mode(axis=0)\n",
    "Autism_frame[meta.names()[12]].replace(np.NaN, autism_mode[meta.names()[12]].values[0],inplace = True)\n",
    "Autism_frame[meta.names()[19]].replace(np.NaN, autism_mode[meta.names()[19]].values[0],inplace = True)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelEncoder_X = LabelEncoder()\n",
    "for i in range(11,len(meta.names())):\n",
    "    if i != 17: \n",
    "        Autism_frame[meta.names()[i]] = labelEncoder_X.fit_transform(Autism_frame[meta.names()[i]])\n",
    "\n",
    "autism_data = Autism_frame.iloc[:,:-1].values\n",
    "autism_labels = Autism_frame.iloc[:,20].values\n",
    "autism_data = autism_data.astype(int, copy = True)\n",
    "autism_labels = autism_labels.astype(int, copy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the top r principal components to obtain 100% of the original data information and transforming the data into the r-dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define PCA Function\n",
    "def pca(data, num_of_prin_comp, data_orientation = \"row\"):\n",
    "    num_of_data = len(data)\n",
    "    dim_of_data = len(data[0])\n",
    "    if data_orientation == \"row\":\n",
    "        transposed_data = np.transpose(data) #Changes dataset so that data samples are column vectors\n",
    "    mean = transposed_data.mean(1)  #Mean Vector\n",
    "    centered_data = np.zeros((dim_of_data,num_of_data))\n",
    "\n",
    "    for i in range(num_of_data):\n",
    "        centered_data[:,i] = transposed_data[:,i] - mean  #Centering Data\n",
    "\n",
    "    svd_u, svd_sigma, svd_v = np.linalg.svd(centered_data, full_matrices = True)  # Singular Value Decompostion\n",
    "\n",
    "    u = np.zeros((dim_of_data,num_of_prin_comp))\n",
    "    s = np.zeros((num_of_prin_comp,num_of_prin_comp))\n",
    "\n",
    "    for i in range(dim_of_data):\n",
    "        for j in range(num_of_prin_comp):\n",
    "            u[i,j] = svd_u[i,j] #First r singular vectors of U\n",
    "    for i in range(num_of_prin_comp):\n",
    "        s[i,i] = svd_sigma[i] #Largest r singular values\n",
    "    \n",
    "    w = np.matrix(u)*np.matrix(s) #Principal Component Matrix with Principal Axes as Columns\n",
    "    for i in range(num_of_prin_comp):\n",
    "        w[:,i] = w[:,i]/np.linalg.norm(w[:,i]) #Normalizing Each Principal Component\n",
    "\n",
    "\n",
    "    transformed_data = np.transpose(np.transpose(w)*centered_data) #Feature Vectors\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% of the Original Data is represented by the top 4 principal components\n"
     ]
    }
   ],
   "source": [
    "#Finding the smallest number of principal components for .99 Representation of Original Data:\n",
    "m = len(autism_data[0])\n",
    "n = len(autism_data)\n",
    "centered = np.zeros((m,n))\n",
    "for i in range(n):\n",
    "    #Centering Training Data\n",
    "    centered[:,i] = np.transpose(autism_data)[:,i] - np.transpose(autism_data).mean(1)\n",
    "training_data_norm_squared = np.square(np.linalg.norm(centered))\n",
    "svd_u, svd_sigma, svd_v = np.linalg.svd(centered, full_matrices = True)  # SVD\n",
    "\n",
    "r = 0 #Top r principal components\n",
    "\n",
    "for i in range(len(svd_sigma)):\n",
    "    sum = 0\n",
    "    representation = 0;\n",
    "    for j in range(i+1):\n",
    "        sum += np.square(svd_sigma[j])\n",
    "    representation = sum/training_data_norm_squared\n",
    "    if representation >= 1:\n",
    "        r = i+1\n",
    "        print(\"100% of the Original Data is represented by the top\", r, \"principal components\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_autism = pca(autism_data, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the decision trees and the metrics used. Also defining the cross validation function and training the data with 10 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gini Index Metric for Classification\n",
    "def gini_index(region_labels):\n",
    "    return 1-region_labels.tolist().count(stats.mode(region_labels)[0])/len(region_labels)\n",
    "\n",
    "def mean_squared_error(region_labels):\n",
    "    error = 0\n",
    "    mean = np.mean(region_labels)\n",
    "    for i in range(len(region_labels)):\n",
    "        error += np.square(region_labels[i]-mean)\n",
    "    return error/len(region_labels)\n",
    "\n",
    "def predict(region_labels,type_of_problem=\"classification\"):\n",
    "    if type_of_problem == \"classification\":\n",
    "        return stats.mode(region_labels)[0]\n",
    "    elif type_of_problem == \"regression\":\n",
    "        return np.mean(region_labels)\n",
    "    return np.NaN\n",
    "\n",
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.parent = None #Contains the original region this region was split from\n",
    "        self.left = None #Contains the regions that are split from this region\n",
    "        self.right = None\n",
    "        self.prediction = np.NaN #Contains either the majority vote or mean of the data in the region\n",
    "        self.data = None #Contains the dataset of the region\n",
    "        self.labels = None #Contains the labels of the dataset of the region\n",
    "        self.split_axis = np.NaN #The axis of the training data the split was made on\n",
    "        self.split_value = np.NaN #The value on the axis where the training data was split on\n",
    "        \n",
    "def tree_split(region_node, metric, type_of_problem):\n",
    "    data, labels = region_node.data, region_node.labels\n",
    "    left_node, right_node = Node(), Node()\n",
    "    splits, error = np.empty((len(data[0]), 15)), np.empty((len(data[0]), 15))\n",
    "    \n",
    "    for i in range(len(data[0])):\n",
    "        effective_range = np.delete(data[:,i],np.concatenate((np.argpartition(data[:,i], -5)[-5:],np.argpartition(data[:,i], 5)[:5])))\n",
    "        dmin,mean,dmax = np.min(effective_range), np.mean(effective_range), np.max(effective_range)\n",
    "        splits[i] = np.delete(np.linspace(dmin, dmax, num=17),[0,16])\n",
    "        \n",
    "        for j in range(len(splits)):\n",
    "            first, second = list(), list()\n",
    "            for k in range(len(data)):\n",
    "                if (data[k,i] < splits[i,j]):\n",
    "                    first.append(labels[k])\n",
    "                else:\n",
    "                    second.append(labels[k])\n",
    "            if len(first) == 0 or len(second) == 0:\n",
    "                error[i,j] = np.inf\n",
    "            else:\n",
    "                error[i,j] = metric(np.array(first)) + metric(np.array(second))\n",
    "                \n",
    "    region_node.split_axis, s = np.unravel_index(error.argmin(), error.shape)\n",
    "    region_node.split_value = splits[region_node.split_axis, s]\n",
    "    \n",
    "    left_node.data, right_node.data = np.empty((0,len(data[0]))),np.empty((0,len(data[0])))\n",
    "    left_node.labels, right_node.labels = np.empty((0,1)),np.empty((0,1))\n",
    "\n",
    "    for k in range(len(data)):\n",
    "        if (data[k, region_node.split_axis] < region_node.split_value):\n",
    "            left_node.data = np.append(left_node.data, [data[k]], axis=0)\n",
    "            left_node.labels = np.append(left_node.labels, [labels[k]])\n",
    "        else:\n",
    "            right_node.data = np.append(right_node.data, [data[k]], axis=0)\n",
    "            right_node.labels = np.append(right_node.labels, [labels[k]])\n",
    "            \n",
    "    left_node.prediction = predict(left_node.labels,type_of_problem)\n",
    "    right_node.prediction = predict(right_node.labels,type_of_problem)\n",
    "    \n",
    "    region_node.left, region_node.right = left_node, right_node\n",
    "    left_node.parent, right_node.parent = region_node, region_node\n",
    "    \n",
    "    if len(left_node.labels) > 50:\n",
    "        tree_split(left_node,gini_index,type_of_problem)\n",
    "    if len(right_node.labels) > 50:\n",
    "        tree_split(right_node,gini_index,type_of_problem)\n",
    "            \n",
    "#Decision Tree Algorithm\n",
    "def decision_tree(training_data, training_labels, test_data, type_of_problem=\"classification\"):\n",
    "    root = Node()\n",
    "    root.data, root.labels = training_data, training_labels\n",
    "    tree_split(root,gini_index,type_of_problem)\n",
    "    \n",
    "    predicted_values = np.empty((len(test_data), 1))\n",
    "    classification_rate = 0\n",
    "    temp_node = root\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        while (temp_node.left != None and temp_node.right != None):\n",
    "            if test_data[i,temp_node.split_axis] < temp_node.split_value:\n",
    "                temp_node = temp_node.left\n",
    "            else:\n",
    "                temp_node = temp_node.right\n",
    "        predicted_values[i] = temp_node.prediction\n",
    "    return predicted_values    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def random_forests(train, train_lbls, test):\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(train,train_lbls)\n",
    "    return clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(k, split_data, split_labels, classifier_func = decision_tree):\n",
    "    classification_rate = np.zeros((k)) #Array to hold classification rate of each fold\n",
    "    \n",
    "    for i in range(k):\n",
    "        #Seperating split data into training and test sets (4 splits for training, 1 for testing)\n",
    "        train = np.concatenate(np.delete(split_data, i))\n",
    "        train_lbls = np.concatenate(np.delete(split_labels, i))\n",
    "        \n",
    "        test = split_data[i]\n",
    "        test_lbls = split_labels[i]\n",
    "        \n",
    "        #Obtaining classified test labels\n",
    "        classifier_labels = classifier_func(train, train_lbls, test)\n",
    "        \n",
    "        #Calculating classification rate: (# of correctly classified test samples)/(total number of test samples)\n",
    "        for j in range(len(classifier_labels)):\n",
    "            if test_lbls[j] == classifier_labels[j]:\n",
    "                classification_rate[i] += 1\n",
    "        classification_rate[i] /= len(classifier_labels)\n",
    "    \n",
    "    #Returning the mean and variance of the classification rates\n",
    "    return np.mean(classification_rate), np.var(classification_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits\n",
    "k = 10 #Number of folds for cross validation\n",
    "\n",
    "n = len(transformed_autism)\n",
    "\n",
    "data_splits = np.zeros((k), dtype=object)\n",
    "label_splits = np.zeros((k), dtype=object)\n",
    "\n",
    "#Defining the splits\n",
    "for i in range(0,k):\n",
    "    data_splits[i] = np.array(transformed_autism[int((i*n)/k): int((i+1)*n/k)])\n",
    "    label_splits[i] = np.array(autism_labels[int((i*n)/k): int((i+1)*n/k)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of classification rate with decision trees is:  0.7320925553319919\n",
      "Variance of classification rate with decision trees is:  0.011773702577638873\n"
     ]
    }
   ],
   "source": [
    "mean, var = cross_validation(10, data_splits, label_splits, decision_tree)\n",
    "print(\"Mean of classification rate with decision trees is: \", mean)\n",
    "print(\"Variance of classification rate with decision trees is: \", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing with sklearn's implementation of Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of classification rate with random forests is:  0.9928973843058351\n",
      "Variance of classification rate with random forests is:  0.00013094300207684722\n"
     ]
    }
   ],
   "source": [
    "mean, var = cross_validation(10, data_splits, label_splits, random_forests)\n",
    "print(\"Mean of classification rate with random forests is: \", mean)\n",
    "print(\"Variance of classification rate with random forests is: \", var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
