{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and Preprocessing the Adult Autism Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy import stats \n",
    "from scipy.io.arff import loadarff\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#Autism\n",
    "Autism_Adult, meta = loadarff('Autism-Adult-Data.arff')\n",
    "\n",
    "Autism_Adult_data = np.array(Autism_Adult[meta.names()[0]].astype(int, copy = True)).reshape(704,1)\n",
    "\n",
    "# Add every integer input vector to Eye_State_data\n",
    "for i in range(1,11):\n",
    "    Autism_Adult_data = np.c_[Autism_Adult_data, np.array(Autism_Adult[meta.names()[i]]).astype(int, copy = True)]\n",
    "\n",
    "# Add every string input vector to Eye_State_data\n",
    "for i in range(11,17):\n",
    "    Autism_Adult_data = np.c_[Autism_Adult_data, np.array(Autism_Adult[meta.names()[i]]).astype(str, copy = True)]\n",
    "\n",
    "# Add integer input vector to Eye_State_data, 18th column\n",
    "Autism_Adult_data = np.c_[Autism_Adult_data, np.array(Autism_Adult[meta.names()[17]]).astype(int, copy = True)]\n",
    "\n",
    "for i in range(18,len(meta.names())):\n",
    "    Autism_Adult_data = np.c_[Autism_Adult_data, np.array(Autism_Adult[meta.names()[i]]).astype(str, copy = True)]\n",
    "\n",
    "Autism_frame = pd.DataFrame(data = Autism_Adult_data, columns = meta.names()[:])\n",
    "Autism_frame.replace('?',np.NaN, inplace = True)\n",
    "\n",
    "autism_mode = Autism_frame.mode(axis=0)\n",
    "Autism_frame[meta.names()[12]].replace(np.NaN, autism_mode[meta.names()[12]].values[0],inplace = True)\n",
    "Autism_frame[meta.names()[19]].replace(np.NaN, autism_mode[meta.names()[19]].values[0],inplace = True)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelEncoder_X = LabelEncoder()\n",
    "for i in range(11,len(meta.names())):\n",
    "    if i != 17: \n",
    "        Autism_frame[meta.names()[i]] = labelEncoder_X.fit_transform(Autism_frame[meta.names()[i]])\n",
    "        \n",
    "Autism_frame.drop(columns = ['age_desc'], inplace = True)\n",
    "Autism_frame.drop(columns = ['result'], inplace = True)\n",
    "Autism_frame.drop(columns = ['used_app_before'], inplace = True)\n",
    "Autism_frame.drop(columns = ['contry_of_res'], inplace = True)\n",
    "\n",
    "autism_data = Autism_frame.iloc[:,:-1].values\n",
    "autism_labels = Autism_frame.iloc[:,16].values\n",
    "autism_data = autism_data.astype(int, copy = True)\n",
    "autism_labels = autism_labels.astype(int, copy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the top r principal components to obtain 99.99% of the original data information and transforming the data into the r-dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define PCA Function\n",
    "def pca(data, num_of_prin_comp, data_orientation = \"row\"):\n",
    "    num_of_data = len(data)\n",
    "    dim_of_data = len(data[0])\n",
    "    if data_orientation == \"row\":\n",
    "        transposed_data = np.transpose(data) #Changes dataset so that data samples are column vectors\n",
    "    mean = transposed_data.mean(1)  #Mean Vector\n",
    "    centered_data = np.zeros((dim_of_data,num_of_data))\n",
    "\n",
    "    for i in range(num_of_data):\n",
    "        centered_data[:,i] = transposed_data[:,i] - mean  #Centering Data\n",
    "\n",
    "    svd_u, svd_sigma, svd_v = np.linalg.svd(centered_data, full_matrices = True)  # Singular Value Decompostion\n",
    "\n",
    "    u = np.zeros((dim_of_data,num_of_prin_comp))\n",
    "    s = np.zeros((num_of_prin_comp,num_of_prin_comp))\n",
    "\n",
    "    for i in range(dim_of_data):\n",
    "        for j in range(num_of_prin_comp):\n",
    "            u[i,j] = svd_u[i,j] #First r singular vectors of U\n",
    "    for i in range(num_of_prin_comp):\n",
    "        s[i,i] = svd_sigma[i] #Largest r singular values\n",
    "    \n",
    "    w = np.matrix(u)*np.matrix(s) #Principal Component Matrix with Principal Axes as Columns\n",
    "    for i in range(num_of_prin_comp):\n",
    "        w[:,i] = w[:,i]/np.linalg.norm(w[:,i]) #Normalizing Each Principal Component\n",
    "\n",
    "\n",
    "    transformed_data = np.transpose(np.transpose(w)*centered_data) #Feature Vectors\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\sum_{i=1}^{k} \\sigma^2_i}{\\left \\| \\overline{X} \\right \\|^2_F}\\geq .9999$\n",
    "<br>\n",
    "This uses the singular value decomposition and the correlation of representation to the singular vlaues to find the top r principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.03268355e+09 9.71392010e+01 2.13240272e+01 1.75560118e+01\n",
      " 1.45167897e+01 1.34245513e+01 1.27766248e+01 1.18975048e+01\n",
      " 1.15093056e+01 1.12745755e+01 1.03558018e+01 1.00589872e+01\n",
      " 9.85292466e+00 8.81945541e+00 8.44445192e+00 7.39238642e+00]\n",
      "99.99% of the Original Data is represented by the top 1 principal components\n"
     ]
    }
   ],
   "source": [
    "#Finding the smallest number of principal components for .9999 Representation of Original Data:\n",
    "m = len(autism_data[0])\n",
    "n = len(autism_data)\n",
    "centered = np.zeros((m,n))\n",
    "for i in range(n):\n",
    "    #Centering Training Data\n",
    "    centered[:,i] = np.transpose(autism_data)[:,i] - np.transpose(autism_data).mean(1)\n",
    "training_data_norm_squared = np.square(np.linalg.norm(centered))\n",
    "svd_u, svd_sigma, svd_v = np.linalg.svd(centered, full_matrices = True)  # SVD\n",
    "print(svd_sigma)\n",
    "r = 0 #Top r principal components\n",
    "\n",
    "for i in range(len(svd_sigma)):\n",
    "    sum = 0\n",
    "    representation = 0;\n",
    "    for j in range(i+1):\n",
    "        sum += np.square(svd_sigma[j])\n",
    "    representation = sum/training_data_norm_squared\n",
    "    if representation >= .9999:\n",
    "        r = i+1\n",
    "        print(\"99.99% of the Original Data is represented by the top\", r, \"principal components\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the data using the number of principal components found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_autism = pca(autism_data, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the decision trees and the metrics used. Also defining the cross validation function and training the data with 10 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn's implementation of random forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def random_forests(train, train_lbls, test):\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(train,train_lbls)\n",
    "    return clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(k, split_data, split_labels, classifier_func = random_forests):\n",
    "    classification_rate = np.zeros((k)) #Array to hold classification rate of each fold\n",
    "    \n",
    "    for i in range(k):\n",
    "        #Seperating split data into training and test sets\n",
    "        train = np.concatenate(np.delete(split_data, i))\n",
    "        train_lbls = np.concatenate(np.delete(split_labels, i))\n",
    "        \n",
    "        test = split_data[i]\n",
    "        test_lbls = split_labels[i]\n",
    "        \n",
    "        #Obtaining classified test labels\n",
    "        classifier_labels = classifier_func(train, train_lbls, test)\n",
    "        \n",
    "        #Calculating classification rate: (# of correctly classified test samples)/(total number of test samples)\n",
    "        for j in range(len(classifier_labels)):\n",
    "            if test_lbls[j] == classifier_labels[j]:\n",
    "                classification_rate[i] += 1\n",
    "        classification_rate[i] /= len(classifier_labels)\n",
    "    \n",
    "    #Returning the mean and variance of the classification rates\n",
    "    return np.mean(classification_rate), np.var(classification_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits\n",
    "k = 10 #Number of folds for cross validation\n",
    "\n",
    "n = len(transformed_autism)\n",
    "\n",
    "data_splits = np.zeros((k), dtype=object)\n",
    "data_splits2 = np.zeros((k), dtype=object)\n",
    "label_splits = np.zeros((k), dtype=object)\n",
    "\n",
    "#Defining the splits\n",
    "for i in range(0,k):\n",
    "    data_splits[i] = np.array(transformed_autism[int((i*n)/k): int((i+1)*n/k)])\n",
    "    data_splits2[i] = np.array(autism_data[int((i*n)/k): int((i+1)*n/k)])\n",
    "    label_splits[i] = np.array(autism_labels[int((i*n)/k): int((i+1)*n/k)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn's implementation of Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of classification rate with random forests is:  0.6609255533199195\n",
      "Variance of classification rate with random forests is:  0.007789431154330407\n"
     ]
    }
   ],
   "source": [
    "mean, var = cross_validation(k, data_splits, label_splits, random_forests)\n",
    "print(\"Mean of classification rate with random forests is: \", mean)\n",
    "print(\"Variance of classification rate with random forests is: \", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using random forests on original dataset without feature extraction to compare classification rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of classification rate with random forests and original data is:  0.9376458752515091\n",
      "Variance of classification rate with random forests and original data is:  0.000754753470521317\n"
     ]
    }
   ],
   "source": [
    "mean, var = cross_validation(k, data_splits2, label_splits, random_forests)\n",
    "print(\"Mean of classification rate with random forests and original data is: \", mean)\n",
    "print(\"Variance of classification rate with random forests and original data is: \", var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
